{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cba00bc",
   "metadata": {},
   "source": [
    "# Prac 5:Topic Modelling using LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abde370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bfc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'abcnews-date-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76efc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the publish date.\n",
    "df.drop(['publish_date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68489397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(headline):\n",
    "  le=WordNetLemmatizer()\n",
    "  word_tokens=word_tokenize(headline)\n",
    "  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "  cleaned_text=\" \".join(tokens)\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c468e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline_cleaned_text']=df['headline_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4638134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['headline_text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1567fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decides community broadcasting licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire witness must aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>staff aust strike rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strike affect australian traveller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    headline_cleaned_text\n",
       "0  decides community broadcasting licence\n",
       "1      fire witness must aware defamation\n",
       "2   call infrastructure protection summit\n",
       "3                  staff aust strike rise\n",
       "4      strike affect australian traveller"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09563e9f",
   "metadata": {},
   "source": [
    "# Extracting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72047842",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767dac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_text=vect.fit_transform(df['headline_cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f7a6137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226258, 1000)\n",
      "  (0, 507)\t0.7830964759517771\n",
      "  (0, 180)\t0.6219002406752289\n",
      "  (1, 575)\t0.6350011874790689\n",
      "  (1, 982)\t0.634252507913514\n",
      "  (1, 322)\t0.44101842150367176\n",
      "  (2, 850)\t0.6547003749683041\n",
      "  (2, 681)\t0.6236747415657183\n",
      "  (2, 124)\t0.42707989387150563\n",
      "  (3, 743)\t0.4535400720072263\n",
      "  (3, 842)\t0.4901743730039168\n",
      "  (3, 56)\t0.5225164976545785\n",
      "  (3, 826)\t0.5301009307789318\n",
      "  (4, 58)\t0.6373967041659779\n",
      "  (4, 842)\t0.7705358145591605\n",
      "  (5, 977)\t1.0\n",
      "  (6, 709)\t1.0\n",
      "  (7, 542)\t0.5180441448099484\n",
      "  (7, 345)\t0.48078322560362474\n",
      "  (7, 960)\t0.535924910123952\n",
      "  (7, 55)\t0.46180325325287314\n",
      "  (8, 452)\t0.42931170443114947\n",
      "  (8, 202)\t0.34965626131197547\n",
      "  (8, 775)\t0.4483609681844147\n",
      "  (8, 13)\t0.5219542868074372\n",
      "  (8, 56)\t0.4690075948807512\n",
      "  :\t:\n",
      "  (1226249, 831)\t0.3778999040452349\n",
      "  (1226250, 100)\t0.5663193054263862\n",
      "  (1226250, 941)\t0.503939080598977\n",
      "  (1226250, 548)\t0.4622873009035714\n",
      "  (1226250, 135)\t0.4600198895370982\n",
      "  (1226251, 344)\t0.6080525617065659\n",
      "  (1226251, 969)\t0.7938967704948061\n",
      "  (1226252, 500)\t0.6238436340607845\n",
      "  (1226252, 169)\t0.6216900029959888\n",
      "  (1226252, 57)\t0.4736250208938309\n",
      "  (1226253, 994)\t0.5066405611890687\n",
      "  (1226253, 524)\t0.7105693787979459\n",
      "  (1226253, 63)\t0.48826888050819595\n",
      "  (1226254, 207)\t0.7720691816264106\n",
      "  (1226254, 813)\t0.63553849512262\n",
      "  (1226255, 197)\t0.4709048526948187\n",
      "  (1226255, 941)\t0.5131899033214004\n",
      "  (1226255, 994)\t0.43571949255955134\n",
      "  (1226255, 734)\t0.570116888577325\n",
      "  (1226256, 207)\t0.5078702413212534\n",
      "  (1226256, 511)\t0.5626250693467827\n",
      "  (1226256, 508)\t0.4574039151076636\n",
      "  (1226256, 253)\t0.4650833342176444\n",
      "  (1226257, 127)\t0.7598601186999014\n",
      "  (1226257, 984)\t0.65008660962165\n"
     ]
    }
   ],
   "source": [
    "print(vect_text.shape)\n",
    "print(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664b8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=vect.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17535524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "police walk\n",
      "4.440524277043323\n",
      "7.916979273509669\n"
     ]
    }
   ],
   "source": [
    "dd=dict(zip(vect.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "# print(l)\n",
    "print(l[0],l[-1])\n",
    "print(dd['police'])\n",
    "print(dd['forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc2f03",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3631b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "\n",
    "lsa_top=lsa_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c0af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.53029397e-04  1.01492896e-02  2.19680672e-02 ... -2.21296268e-03\n",
      "   3.17245107e-03  1.32956025e-04]\n",
      " [ 7.48585858e-04  5.08976424e-02  7.87240765e-02 ...  1.18530502e-01\n",
      "  -9.25239380e-02  1.51522464e-02]\n",
      " [ 5.37521234e-04  3.66229969e-02  9.96668480e-02 ...  3.24419227e-01\n",
      "  -4.05173232e-02 -2.13994257e-02]\n",
      " ...\n",
      " [ 7.30007417e-04  3.32113316e-02  6.16869649e-02 ...  6.26249380e-04\n",
      "   6.26462090e-02  4.76153017e-03]\n",
      " [ 2.30190071e-04  1.17542405e-02  2.81349680e-02 ...  2.49101219e-03\n",
      "   1.41661310e-02  1.56714585e-03]\n",
      " [ 9.47397042e-04  7.22237294e-02  7.51528438e-02 ... -1.76142492e-03\n",
      "   2.51522741e-01 -1.62705114e-01]]\n",
      "(1226258, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d6c48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  0.015302939715647319\n",
      "Topic  1  :  1.0149289606607355\n",
      "Topic  2  :  2.1968067180559117\n",
      "Topic  3  :  -0.8919740217810643\n",
      "Topic  4  :  -0.8020581144200037\n",
      "Topic  5  :  -0.677594878379342\n",
      "Topic  6  :  0.06418594298345061\n",
      "Topic  7  :  -0.22129626758997958\n",
      "Topic  8  :  0.31724510712124443\n",
      "Topic  9  :  0.01329560245325193\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc7631bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "interview extended michael john david smith james police andrew mark \n",
      "\n",
      "Topic 1: \n",
      "police death woman fire crash court say probe call murder \n",
      "\n",
      "Topic 2: \n",
      "say australia plan council call back govt fire court water \n",
      "\n",
      "Topic 3: \n",
      "say police need trump minister must expert wont needed hunt \n",
      "\n",
      "Topic 4: \n",
      "australia south world police first test china coronavirus india cricket \n",
      "\n",
      "Topic 5: \n",
      "court face woman murder fire charged charge accused death crash \n",
      "\n",
      "Topic 6: \n",
      "fire house sydney home crew govt australia plan school damage \n",
      "\n",
      "Topic 7: \n",
      "call fire say medium home house death australian inquiry spark \n",
      "\n",
      "Topic 8: \n",
      "australian woman crash back dy year open killed charged world \n",
      "\n",
      "Topic 9: \n",
      "back court police fire australian fight world hit face school \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
